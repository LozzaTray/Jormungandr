\section{Appendix}

\subsection{Derivation of {\boldmath $p(b|X)$}}
\label{appdx:b|x}

We determine the form of $p(b| X)$ by integrating
out the parameters $\theta$. From the definitions, we have:
%
\begin{align*}
	p(b | X) 
	&= \int p(b , \theta| X) d\theta 
	= \int p(b | X, \theta) p(\theta | X) d\theta
	= \int \prod_{i \in [N] } \phi_{b_i}(x_i; \theta) p(\theta) d\theta \\
	&= \prod_{i \in [N]} \int \frac{\exp(w_{b_i}^T x_i) \prod_{j \in [B]} \Gaussian(w_j; 0, \sigma_\theta^2 I)}{\sum_{k \in [B]} \exp(w_{k}^T x_i)} dw_{1:B}.
\end{align*}
%
The key observation here is that 
the value of the integral is independent
of the value of $b_i \in [B]$ as
the integrand has the same form regardless of $b_i$. This is
because the prior is the same for each $w_j$. 
Therefore, the integral can only be a function of at most $x_i$ and $\sigma_\theta^2$,
which means that, as a function of $b$, $p(b|X)\propto 1$. As
$b$ takes values in $[B]^N$, we necessarily have:
%
\begin{equation}
	p(b | X) = \frac{1}{\big|[B]^N\big|}=B^{-N}.
\end{equation}

\subsection{Derivation of {\boldmath $U(\theta)$} and {\boldmath $\nabla U(\theta)$}}
\label{appdx:form-U}

Recall from~(\ref{eq:U}) in Section~\ref{s:sfb} that,
$$	\pi_\theta(\theta) \propto p(\theta | X, b) \propto p(b | X, \theta) p(\theta) \propto  \exp \left( - U(\theta) \right),
$$ 
so that $U$ can be expressed as,
$$
U(\theta) 
= - \left( \log p(b | X, \theta) + \log p(\theta) \right) + \textrm{const}.
$$
Writing,
$y_{ij} \coloneqq \one \left\{ b_i = j \right\}$ and 
$a_{ij} \coloneqq \phi_j(x_i; \theta)$, we have that,
%
\begin{equation*}
	\log p(b | X, \theta) = \sum_{i \in [N]} \sum_{j \in [B]} y_{ij} \log a_{ij}  \quad \textrm{and} \quad
	\log p(\theta) = -\frac{D B}{2} \log 2\pi - \frac{1}{2 \sigma_\theta^2} 
	\|\theta \|^2,
	\label{eqn:U-constituent-terms}
\end{equation*}
%
where
$\|\theta\|^2 = \sum_{i} \theta_{i}^2 = \sum_{j \in [B]} \|w_j\|^2$ 
is the Euclidean norm of the vector of parameters $\theta$.
Therefore, discarding constant terms, we 
obtain,
%
\begin{equation}
	U(\theta) = \left( \sum_{i \in [N]} \sum_{j \in [B]} y_{ij} \log \frac{1}{a_{ij}} \right)
	+ \frac{1}{2\sigma_\theta^2} \|\theta\|^2.
\end{equation}

As
$\theta = \{ w_k \}_{k=1}^{B}$,
to compute the gradient
$\nabla U(\theta)$ we need to compute
each of its components,
$\nicefrac{\partial U}{\partial w_k}$,
$1\leq k\leq B$.
To that end, we first compute,
%
\begin{align}
	\frac{\partial a_{ij}}{\partial w_k} &= \frac
	{x_i \exp(w_j^T x_i) \delta_{jk} \cdot \sum_{r \in [B]} \exp(w_r^T x_i) 
		- 
		\exp(w_j^T x_i) \cdot x_i \exp(w_k^T x_i)}
	{\left( \sum_{r \in [B]} \exp(w_r^T x_i) \right)^2} \nonumber \\
	&= x_i \left( a_{ij} \delta_{jk} - a_{ij}a_{ik} \right), 
	\label{eq:dadw}
\end{align}
%
where $\delta_{jk} \coloneqq \one \left\{ j = k \right\}$,
and we also easily find,
%
\begin{equation}
	\frac{ \partial}{\partial w_k} \|\theta\|^2 = \frac{\partial}{\partial w_k} \left( \sum_{r \in [B]} \|w_r\|^2 \right) = 2w_k.
	\label{eq:dtsdw}
\end{equation}
%
Recall the expression for $U(\theta)$ in~(\ref{eqn:U-form}). Using~(\ref{eq:dadw}) and~(\ref{eq:dtsdw}), we obtain,
\begin{align}
	\frac{\partial U}{\partial w_k} &= 
	\sum_{i \in [N]} \sum_{j \in [B]} y_{ij} 
	\left( -\frac{x_i}{a_{ij}} \left( a_{ij} \delta_{jk} - a_{ij} a_{ik} \right) \right)
	+ \frac{w_k}{\sigma_\theta^2} \nonumber \\
	&= - \left( \sum_{i \in [N]} \Big\{ x_i (y_{ik} - a_{ik}) \Big\} - \frac{w_k}{\sigma_\theta^2} \right).
\end{align}
%
This can be computed 
efficiently through matrix operations. The only property of $y_{ij}$ 
we have used in the derivation is the constraint $\sum_{j \in [B]} y_{ij} = 1$,
for all $i$.

\subsection{Implementation details}
\label{appdx:hyperparams}

All data analysis and visualisation was implemented in Python. Full source code is available at:
\url{https://github.com/LozzaTray/Jormungandr-code}. Table~\ref{tab:hyperparams} contains the hyper-parameters for each experiment run. The set of retained samples are generated as,
$
	\Tcal_\star = \{T_\star \kappa_\star + i \lambda_\star :  
	0 \leq i \leq \lfloor T_\star(1 - \kappa_\star) / \lambda_\star \rfloor \},
$
with $\kappa_star$ controlling burn-in and $\lambda_star$ controlling thinning.

\begin{table}[!h]
	\centering
	\caption{Hyper-parameter values for each experiment}
	\label{tab:hyperparams}
	\resizebox{\textwidth}{!}{%
		\begin{tabular}{c|ccc|ccc|ccc|cc|ccc}
			Dataset & 
			$B$ & $f$ & $\sigma_\theta$ & 
			$T_b$ & $\kappa_b$ & $\lambda_b$ & 
			$T_\theta$ & $\kappa_\theta$ & $\lambda_\theta$ & %$s$ &
			$k$ & $D'$ &
			$T_\theta'$ & $\kappa_\theta'$ & $\lambda_\theta'$ %& $s'$
			\\ \hline
			Polbooks &
			3 & 0.7 & 1 &
			1,000 & 0.2 & 5 &
			10,000 & 0.4 & 10 & %0.05 &
			-- & -- & 
			-- & -- & -- %& -- 
			\\
			School &
			10 & 0.7 & 1 &
			1,000 & 0.2 & 5 &
			10,000 & 0.4 & 10 & %0.2 &
			1 & 10 & 
			10,000 & 0.4 & 10 %& 0.2 
			\\
			FB Egonet &
			10 & 0.7 & 1 &
			1,000 & 0.2 & 5 &
			10,000 & 0.4 & 10 & %0.017 &
			1 & 10 & 
			10,000 & 0.4 & 10 %& 0.5 
			\\
		\end{tabular}
	}
\end{table}