\section{Experiments}
\label{sec:experiments}

We apply the developed methods to a vareity of datasets chosen to span a range of edge counts $E$ and feature-space dimension $D$. 
However, we first require metrics to assess model performance. This can be split into two separate 
components: the microcanonical SBM fit (concerned with the $b$-samples) and 
the fit of the feature-to-block generator (concerned with the $\theta$-samples). 

Starting with the SBM, recall that the quantity $S(b)$ in~(\ref{eqn:dl-form}) 
can be interpreted as an ideal ``description length'' of the partition 
imposed by $b$. We define the average
description length per entity (nodes and edges) $\bar{S}_e$ to gauge the SBM fit:
%
\begin{equation}
	\bar{S}_e \coloneqq \frac{1}{(N+E) |\Tcal_b|} \sum_{t\in \Tcal_b} S \left( b^{(t)} \right).
	\label{eqn:mean-dl}
\end{equation}

Next, to assess the performance of the feature-to-block predictor, 
we partition the vertex set $[N]$ into training and test sets. On each experiment run,
a constant fraction $f$ of the available vertices go to form 
the training set $\Gcal_0$ and the remainder are held out to form the
test set $\Gcal_1$. These sets are chosen randomly.
The $b$-chain is run using the whole network but we only use vertices $v \in \Gcal_0$ to train the $\theta$-chain. Because $|\Gcal_0| \neq |\Gcal_1|$ in general, we cannot use the un-normalised log-target $U$ from~(\ref{eqn:U-form}) for benchmarking,
as the total cross-entropy loss scales with the size of each data set but 
the prior term stays constant. We therefore use the average cross-entropy loss 
over each set,
%
\begin{equation}
	\bar{\Lcal}_\star \coloneqq \frac{1}{|\Tcal_\theta|} \sum_{t \in \Tcal_\theta} \Lcal_\star^{(t)},
	\quad \textrm{where} \quad
	\Lcal_\star^{(t)} \coloneqq \frac{1}{|\Gcal_\star|} \sum_{i \in \Gcal_\star}\sum_{j \in [B]} \hat{y}_{ij} \log \frac{1}{\phi_j \left(x_i; \theta^{(t)} \right)},
	\label{eqn:cross-entropy-loss}
\end{equation}
%
where $\star \in \{0, 1\}$ indicates whether the training or test
set is being considered.

Nevertheless, the cross-entropy loss over the whole training set may be too coarse a measure of model fit. It is often the case that we have good feature-based explanations for some but not all of the detected blocks. We wish to define a new measure of fit specific to each detected block. This requires defining the set of all vertices associated with block $j$ as,
$
	\Bcal_\star(j) \coloneqq \{i \in \Gcal_\star : \hat{b}_i = j\}
$
where
$ 
	\quad \hat{b}_i \coloneqq \argmax_j \hat{y}_{ij}.
$
Recall that $\hat{y}_{ij}$ is our estimate for the block membership posterior~(\ref{eqn:y-hat}) using only information from the adjacency matrix $A$. Again, $\star \in \{0, 1\}$ toggles between training and test sets. Now $\Bcal_\star(j)$ is the set of vertices that have maximum a posteriori probability of belonging to block $j$. We choose to resolve ties consistently by picking the lower index. We now define the accuracy for block $j$ as,
%
\begin{equation}
	\eta_\star(j) \coloneqq \frac{1}{|\Bcal_\star (j)| \cdot 
	|\Tcal_\theta| } 
	\sum_{i \in \Bcal_\star (j)}  \sum_{t \in \Tcal_\theta}
	\one \left\{\hat{b}_i = \argmax_j \phi_j \left( x_i; \theta^{(t)} \right) \right\}.
	\label{eqn:accuracy}
\end{equation}
%
This is effectively testing whether the feature-to-block and the graph-to=block predictions agree in their largest component. We call this metric, $\eta_\star(j)$, the {\em block-accuracy} for block $j$. It is clearly bounded $0 \leq \eta_\star(j) \leq 1$, with an accuracy of 1 meaning perfect agreement for the vertices in detected block $j$.

For each of the experiments, we plot the collected $\theta$-samples for the features that survive the dimensionality reduction procedure. We also plot the per-block accuracy for the original-dimension classifiers. We discuss the results specific to each dataset in turn.

\input{experiments-detailed}
\input{experiments-figures}