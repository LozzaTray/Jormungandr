\section{Experimental results}
\label{sec:experiments}

We apply the developed methods to a variety of datasets. These are chosen to span a range of node counts $N$, edge counts $E$ and feature space dimension $D$.
%
We require metrics to assess performance. This can be split into two separate 
components: the microcanonical SBM fit (concerned with the $b$-samples) and 
the fit of the feature-to-block generator (concerned with the $\theta$-samples). 

Starting with the SBM, recall that the quantity $S(b)$ in~(\ref{eqn:dl-form}) 
can be interpreted as an ideal ``description length'' of the partition 
imposed by $b$. We define a simple
metric $\bar{S}_e$ to gauge the fit of the SBM,
as the description length per entity 
(i.e., divided by the total number $N+E$ of 
nodes plus edges),
averaged over the $b$-samples:
%
\begin{equation}
	\bar{S}_e \coloneqq \frac{1}{(N+E) |\Tcal_b|} \sum_{t\in \Tcal_b} S \left( b^{(t)} \right).
	\label{eqn:mean-dl}
\end{equation}
%

Next, to assess the performance of the feature-to-block predictor, 
we partition the vertex set $[N]$ into training and test sets. We choose to 
randomly partition the vertices on each experiment run,
so that a constant fraction $f$ of the available vertices go to form 
the training set $\Gcal_0$ and the remainder are held out to form the
test set $\Gcal_1$.
The $b$-chain is run using the whole network but we only use vertices $v \in \Gcal_0$ to train the $\theta$-chain. Because $|\Gcal_0| \neq |\Gcal_1|$ in general, we cannot use the un-normalised log-target $U$ from~(\ref{eqn:U-form}) for comparison,
as the total cross-entropy loss scales with the size of each data set but 
the prior term stays constant. We therefore use the average cross-entropy loss 
over each set,
%
\begin{equation}
	\bar{\Lcal}_\star \coloneqq \frac{1}{|\Tcal_\theta|} \sum_{t \in \Tcal_\theta} \Lcal_\star^{(t)},
	\quad \textrm{where} \quad
	\Lcal_\star^{(t)} \coloneqq \frac{1}{|\Gcal_\star|} \sum_{i \in \Gcal_\star}\sum_{j \in [B]} \hat{y}_{ij} \log \frac{1}{\phi_j \left(x_i; \theta^{(t)} \right)},
	\label{eqn:cross-entropy-loss}
\end{equation}
%
where $\star \in \{0, 1\}$ indicates whether the training or test
set is being considered.

Table~\ref{tab:results} summarises the results for each experiment. We also apply the 
dimensionality reduction method 
of Section~\ref{sec:dim-reduction}
to the two higher dimensional datasets (the school and FB egonet). 
For this we use equation~(\ref{eqn:c-star}) with $k=1$,
in order to reduce the dimension from 
$D$ to a desired $D'$. 
We then retrain the feature-block predictor using only the retained 
feature set $\Dcal'$, and report the log-loss over the training and 
test sets for the reduced classifier -- 
denoted $\bar{\Lcal}_0'$ and $\bar{\Lcal}_1'$ respectively. 
These values are also given in Table~\ref{tab:results}.

Based on these results, some remarks are in order.
Firstly, the variance of the test loss $\bar{\Lcal}_1$ tends to be higher 
than the training loss $\bar{\Lcal}_0$. This is expected,
as the test set is smaller than the training set and hence 
more susceptible to variability in its construction. 
Secondly, it can be seen that the dimensionality reduction procedure 
brings the training and test losses closer together. This implies that 
the features we keep are indeed correlated with the underlying graphical 
partition and that the approach generalises correctly.

The average description length per entity,
$\bar{S}_e$, of the graph, 
has very small variance, suggesting that
the detected communities can be found reliably (to within an arbitrary 
relabelling of blocks). For reference, we plot an inferred partition for each 
of the graphs in Figure~\ref{fig:graphs-all}. The polbooks graph yields the cleanest separation between blocks but nonetheless the inferred partitions for the other datasets do succeed at dividing the graph into densely connected clusters.

Nevertheless, the cross-entropy loss over the whole training set may be too coarse a measure of model fit. It is often the case that we have good feature-based explanations for some but not all of the detected blocks. We wish to define a new measure of fit specific to each detected block. This requires defining the set of all vertices associated with block $j$ as,
$
	\Bcal_\star(j) \coloneqq \{i \in \Gcal_\star : \hat{b}_i = j\}
$
where
$ 
	\quad \hat{b}_i \coloneqq \argmax_j \hat{y}_{ij}.
$
Recall that $\hat{y}_{ij}$ is our estimate for the block membership posterior~(\ref{eqn:y-hat}) using only information from the adjacency matrix $A$. Again, $\star \in \{0, 1\}$ toggles between training and test sets. Now $\Bcal_\star(j)$ is the set of vertices that have maximum a posteriori probability of belonging to block $j$. We choose to resolve ties consistently by picking the lower index. We now define the accuracy for block $j$ as,
%
\begin{equation}
	\eta_\star(j) \coloneqq \frac{1}{|\Bcal_\star (j)| \cdot 
	|\Tcal_\theta| } 
	\sum_{i \in \Bcal_\star (j)}  \sum_{t \in \Tcal_\theta}
	\one \left\{\hat{b}_i = \argmax_j \phi_j \left( x_i; \theta^{(t)} \right) \right\}.
	\label{eqn:accuracy}
\end{equation}
%
This is effectively testing whether the feature-to-block and the graph-to=block predictions agree in their largest component. We call this metric, $\eta_\star(j)$, the {\em block-accuracy} for block $j$. It is clearly bounded $0 \leq \eta_\star(j) \leq 1$, with an accuracy of 1 meaning perfect agreement for the vertices in detected block $j$.

For each of the experiments, we plot the collected $\theta$-samples for the features that survive the dimensionality reduction procedure. We also plot the per-block accuracy for the original-dimension classifiers. We discuss the results specific to each dataset in turn.

\input{experiments-detailed}
\input{experiments-figures}